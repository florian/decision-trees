{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "\n",
    "A popular method to combat overfitting in decision trees is random forest. We'll first implement a simple majority classifier and will then use it to build a random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from C45 import C45, mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MajorityClassifier\n",
    "\n",
    "This classifier takes a set of already fitted model. To predict the label for new data points, all models are asked and the answer with a relative majority is predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "class MajorityClassifier:\n",
    "    def __init__(self, models):\n",
    "        self.models = models\n",
    "        \n",
    "    def predict_single(self, x):\n",
    "        ys = [model.predict_single(x) for model in self.models]\n",
    "        return Counter(ys).most_common()[0][0]\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return [self.predict_single(x) for x in X]\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        Returns the accuracy for predicting the given dataset X\n",
    "        \"\"\"\n",
    "        \n",
    "        correct = sum(self.predict(X) == y)\n",
    "        return float(correct) / len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomForest\n",
    "\n",
    "Random forest trains a bunch of decision trees on different training data and then gives them to a majority classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class RandomForest(MajorityClassifier):\n",
    "    def __init__(self, num_trees, continuous, max_depth=float(\"inf\")):\n",
    "        self.models = [C45(continuous=continuous, max_depth=max_depth) for _ in range(num_trees)]\n",
    "        \n",
    "    def fit(self, X, y, k=0.1):\n",
    "        num_train = int(len(X) * k)\n",
    "        \n",
    "        for model in self.models:\n",
    "            sub = [random.choice(range(len(X))) for _ in range(num_train)]\n",
    "            X_sub = X[sub]\n",
    "            y_sub = y[sub]\n",
    "            \n",
    "            model.fit(X_sub, y_sub)\n",
    "\n",
    "    def prune(self, X_val, y_val):\n",
    "        for model in self.models:\n",
    "            model.prune(X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Titanic dataset\n",
    "\n",
    "Again, we'll use the titanic dataset to explore how well our algorithm works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "from sklearn.preprocessing import Imputer, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(data, encode_labels=False, impute=False):\n",
    "    X = data.drop([\"Survived\", \"Name\", \"Ticket\", \"Cabin\"], 1)    \n",
    "    \n",
    "    if encode_labels: # for sklearn\n",
    "        X = X.apply(LabelEncoder().fit_transform)\n",
    "    \n",
    "    print X.head(10)\n",
    "    \n",
    "    X = X.as_matrix()\n",
    "    \n",
    "    if impute:\n",
    "        X = Imputer().fit_transform(X)\n",
    "            \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Pclass     Sex   Age  SibSp  Parch     Fare Embarked\n",
      "PassengerId                                                      \n",
      "1                 3    male  22.0      1      0   7.2500        S\n",
      "2                 1  female  38.0      1      0  71.2833        C\n",
      "3                 3  female  26.0      0      0   7.9250        S\n",
      "4                 1  female  35.0      1      0  53.1000        S\n",
      "5                 3    male  35.0      0      0   8.0500        S\n",
      "6                 3    male   NaN      0      0   8.4583        Q\n",
      "7                 1    male  54.0      0      0  51.8625        S\n",
      "8                 3    male   2.0      3      1  21.0750        S\n",
      "9                 3  female  27.0      0      2  11.1333        S\n",
      "10                2  female  14.0      1      0  30.0708        C\n"
     ]
    }
   ],
   "source": [
    "data = DataFrame.from_csv(\"./titanic/train.csv\")\n",
    "y = data[\"Survived\"].as_matrix()\n",
    "X = preprocess(data)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train_sub, X_val, y_train_sub, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=0.02 got 0.75128 accuracy\n",
      "k=0.05 got 0.72279 accuracy\n",
      "k=0.10 got 0.73642 accuracy\n",
      "k=0.15 got 0.75441 accuracy\n",
      "k=0.20 got 0.74855 accuracy\n",
      "k=0.25 got 0.78922 accuracy\n",
      "k=0.30 got 0.78749 accuracy\n",
      "k=0.35 got 0.80285 accuracy\n",
      "k=0.50 got 0.79860 accuracy\n"
     ]
    }
   ],
   "source": [
    "for k in [0.02, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.5]:\n",
    "    clf = RandomForest(num_trees=10, continuous={2, 5})\n",
    "    clf.fit(X_train, y_train, k=k)\n",
    "\n",
    "    acc = mean([clf.score(X_test, y_test) for _ in range(100)])\n",
    "    print \"k=%.2f got %.5f accuracy\" % (k, acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=0.02 got 0.70950 accuracy\n",
      "k=0.05 got 0.78726 accuracy\n",
      "k=0.10 got 0.78212 accuracy\n",
      "k=0.15 got 0.78212 accuracy\n",
      "k=0.20 got 0.79358 accuracy\n",
      "k=0.25 got 0.78212 accuracy\n",
      "k=0.30 got 0.78994 accuracy\n",
      "k=0.35 got 0.78184 accuracy\n",
      "k=0.50 got 0.78531 accuracy\n"
     ]
    }
   ],
   "source": [
    "X_train_sub, X_val, y_train_sub, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=3)\n",
    "\n",
    "for k in [0.02, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.5]:\n",
    "    clf = RandomForest(num_trees=10, continuous={2, 5})\n",
    "    clf.fit(X_train_sub, y_train_sub, k=k)\n",
    "\n",
    "    clf.prune(X_val, y_val)\n",
    "\n",
    "    acc = mean([clf.score(X_test, y_test) for _ in range(100)])\n",
    "    print \"k=%.2f got %.5f accuracy\" % (k, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7788268156424585"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf45 = C45(continuous={2, 5})\n",
    "clf45.fit(X_train, y_train)\n",
    "mean([clf45.score(X_test, y_test) for _ in range(100)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative RandomForest\n",
    "\n",
    "A popular alternative to just splitting the training data is using different features for each trained decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FeatureRandomForest(RandomForest):\n",
    "    def fit(self, X, y, num_features, p=None):\n",
    "        self.num_total_features = X.shape[1]\n",
    "        self.features = {}\n",
    "        \n",
    "        all_features = range(self.num_total_features)\n",
    "        \n",
    "        for model in self.models:\n",
    "            self.features[model] = set(np.random.choice(all_features, size=num_features, p=p, replace=False))\n",
    "            \n",
    "            X_cut = self._cut_data(model, X)\n",
    "            \n",
    "            model.fit(X_cut, y)\n",
    "            \n",
    "    def _cut_data(self, model, X):\n",
    "        features = self.features[model]\n",
    "        cut_features = [feature for feature in range(self.num_total_features) if feature not in features]\n",
    "        \n",
    "        X_cut = X.copy()\n",
    "        X_cut[:, cut_features] = 0\n",
    "        return X_cut\n",
    "    \n",
    "    def prune(self, X_val, y_val):\n",
    "        for model in self.models:\n",
    "            X_cut = self._cut_data(model, X_val)\n",
    "            model.prune(X_cut, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now there's a lot of randomness involved because some trees might only get features that are not useful. With few trees (e.g. two), this is especially bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7988826815642457"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accs = []\n",
    "\n",
    "for _ in range(10):\n",
    "    clf = FeatureRandomForest(num_trees=2, continuous={2, 5})\n",
    "    clf.fit(X_train, y_train, num_features=4)\n",
    "    accs.append(mean([clf.score(X_test, y_test) for _ in range(100)]))\n",
    "\n",
    "max(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.771452513966481,\n",
       " 0.6203910614525137,\n",
       " 0.7430167597765371,\n",
       " 0.7988826815642457,\n",
       " 0.6608938547486031,\n",
       " 0.6770391061452513,\n",
       " 0.7877094972067051,\n",
       " 0.6902234636871508,\n",
       " 0.6757541899441344,\n",
       " 0.6674860335195528]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  With Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forest with 2 features has 0.7758 accuracy\n",
      "forest with 3 features has 0.7901 accuracy\n",
      "forest with 4 features has 0.7307 accuracy\n",
      "forest with 5 features has 0.7842 accuracy\n",
      "forest with 6 features has 0.7961 accuracy\n",
      "forest with 7 features has 0.8214 accuracy\n"
     ]
    }
   ],
   "source": [
    "for num_features in range(2, 8):\n",
    "    clf = FeatureRandomForest(num_trees=10, continuous={2, 5})\n",
    "    clf.fit(X_train_sub, y_train_sub, num_features=num_features)\n",
    "\n",
    "    clf.prune(X_val, y_val)\n",
    "\n",
    "    acc = mean([clf.score(X_test, y_test) for _ in range(100)])\n",
    "    \n",
    "    print \"forest with %d features has %.4f accuracy\" % (num_features, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forest with 7 features has 0.8323 accuracy\n",
      "forest with 6 features has 0.8011 accuracy\n",
      "forest with 5 features has 0.7863 accuracy\n",
      "forest with 4 features has 0.7923 accuracy\n"
     ]
    }
   ],
   "source": [
    "for num_features in reversed(range(4, 8)):\n",
    "    X_train_sub, X_val, y_train_sub, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=3)\n",
    "\n",
    "    clf = FeatureRandomForest(num_trees=10, continuous={2, 5})\n",
    "    clf.fit(X_train_sub, y_train_sub, num_features=num_features)\n",
    "\n",
    "    clf.prune(X_val, y_val)\n",
    "\n",
    "    acc = mean([clf.score(X_test, y_test) for _ in range(100)])\n",
    "    \n",
    "    print \"forest with %d features has %.4f accuracy\" % (num_features, acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forest with 2 features has 0.7020 accuracy\n",
      "forest with 3 features has 0.7563 accuracy\n",
      "forest with 4 features has 0.7229 accuracy\n",
      "forest with 5 features has 0.7943 accuracy\n",
      "forest with 6 features has 0.7527 accuracy\n",
      "forest with 7 features has 0.7837 accuracy\n"
     ]
    }
   ],
   "source": [
    "for num_features in range(2, 8):\n",
    "    clf = FeatureRandomForest(num_trees=10, continuous={2, 5})\n",
    "    clf.fit(X_train, y_train, num_features=num_features)\n",
    "\n",
    "    acc = mean([clf.score(X_test, y_test) for _ in range(100)])\n",
    "    \n",
    "    print \"forest with %d features has %.4f accuracy\" % (num_features, acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prior weights for features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8039106145251403"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pclass     Sex   Age  SibSp  Parch     Fare Embarked\n",
    "p = [0.15, 0.35, 0.1, 0.05, 0.05, 0.2, 0.1]\n",
    "\n",
    "clf = FeatureRandomForest(num_trees=10, continuous={2, 5})\n",
    "clf.fit(X_train_sub, y_train_sub, num_features=4, p=p)\n",
    "clf.prune(X_val, y_val)\n",
    "mean([clf.score(X_test, y_test) for _ in range(100)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7987150837988832"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pclass     Sex   Age  SibSp  Parch     Fare Embarked\n",
    "p = [0.2, 0.3, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
    "\n",
    "clf = FeatureRandomForest(num_trees=10, continuous={2, 5})\n",
    "clf.fit(X_train_sub, y_train_sub, num_features=4, p=p)\n",
    "clf.prune(X_val, y_val)\n",
    "mean([clf.score(X_test, y_test) for _ in range(100)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking with sklearn\n",
    "\n",
    "Our best accuracy is higher than sklearn's, but we did a lot more hyperparameter for our implementation. sklearn's implementation is also a lot less vulnerable to random flunctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Pclass  Sex  Age  SibSp  Parch  Fare  Embarked\n",
      "PassengerId                                                \n",
      "1                 2    1   28      1      0    18         3\n",
      "2                 0    0   51      1      0   207         1\n",
      "3                 2    0   34      0      0    41         3\n",
      "4                 0    0   47      1      0   189         3\n",
      "5                 2    1   47      0      0    43         3\n",
      "6                 2    1  110      0      0    51         2\n",
      "7                 0    1   69      0      0   186         3\n",
      "8                 2    1    6      3      1   124         3\n",
      "9                 2    0   35      0      2    74         3\n",
      "10                1    0   18      1      0   154         1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/numpy/lib/arraysetops.py:200: FutureWarning: numpy not_equal will not check object identity in the future. The comparison did not return the same result as suggested by the identity (`is`)) and will change.\n",
      "  flag = np.concatenate(([True], aux[1:] != aux[:-1]))\n"
     ]
    }
   ],
   "source": [
    "X = preprocess(data, encode_labels=True, impute=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy = 0.97191\n",
      "test accuracy = 0.81564\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "print \"train accuracy = %.5f\" % clf.score(X_train, y_train)\n",
    "print \"test accuracy = %.5f\" % clf.score(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
