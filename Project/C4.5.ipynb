{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C4.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "from sklearn.preprocessing import Imputer, LabelEncoder\n",
    "\n",
    "def preprocess(data, encode_labels=False, impute=False):\n",
    "    X = data.drop([\"Survived\", \"Name\", \"Ticket\", \"Cabin\"], 1)    \n",
    "    \n",
    "    if encode_labels: # for sklearn\n",
    "        X = X.apply(LabelEncoder().fit_transform)\n",
    "    \n",
    "    print X.head()\n",
    "    \n",
    "    X = X.as_matrix()\n",
    "    \n",
    "    if impute:\n",
    "        X = Imputer().fit_transform(X)\n",
    "            \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Pclass  Sex  Age  SibSp  Parch  Fare  Embarked\n",
      "PassengerId                                                \n",
      "1                 2    1   28      1      0    18         3\n",
      "2                 0    0   51      1      0   207         1\n",
      "3                 2    0   34      0      0    41         3\n",
      "4                 0    0   47      1      0   189         3\n",
      "5                 2    1   47      0      0    43         3\n"
     ]
    }
   ],
   "source": [
    "data = DataFrame.from_csv(\"./titanic/train.csv\")\n",
    "y = data[\"Survived\"].as_matrix()\n",
    "X = preprocess(data, encode_labels=True, impute=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = data.drop([\"Survived\", \"Name\", \"Ticket\", \"Cabin\"], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xi = X[\"Age\"].copy().as_matrix()\n",
    "yi = data[\"Survived\"].as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yi = yi[~np.isnan(xi)]\n",
    "xi = xi[~np.isnan(xi)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mean(xs):\n",
    "    return float(sum(xs)) / len(xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "datai = sorted(zip(xi, yi), key=itemgetter(0, 1))\n",
    "\n",
    "splits = []\n",
    "\n",
    "xs = []\n",
    "ys = []\n",
    "last_x = None\n",
    "\n",
    "for xj, yj in datai:\n",
    "    if xj == last_x:\n",
    "        xs[-1].append(xj)\n",
    "        ys[-1].add(yj)\n",
    "    else:\n",
    "        xs.append([xj])\n",
    "        ys.append({yj})\n",
    "        \n",
    "    last_x = xj\n",
    "    \n",
    "last_label = None\n",
    "\n",
    "for xj, yj in zip(xs, ys):\n",
    "    if len(yj) == 1 and list(yj)[0] == last_label:\n",
    "        splits[-1] += xj\n",
    "    else:\n",
    "        splits.append(xj)\n",
    "        \n",
    "    if len(yj) == 1:\n",
    "        last_label = list(yj)[0]\n",
    "    else:\n",
    "        last_label = None\n",
    "        \n",
    "splits = [mean(vals) for vals in splits]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "from math import log as logarithm\n",
    "from operator import itemgetter\n",
    "\n",
    "class C45:\n",
    "    def __init__(self, max_depth=float(\"inf\"), min_gain=0, continuous={}, depth=0):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            max_depth: After eaching this depth, the current node is turned into a leaf which predicts\n",
    "                the most common label. This limits the capacity of the classifier and helps combat overfitting\n",
    "            min_gain: The minimum gain a split has to yield. Again, this helps overfitting\n",
    "            depth: Let's the current node know how deep it is into the tree, users usually don't need to set this\n",
    "        \"\"\"\n",
    "        \n",
    "        self.depth = depth\n",
    "        self.max_depth = max_depth\n",
    "        self.min_gain = min_gain\n",
    "        self.continuous = continuous\n",
    "        \n",
    "        # ID3 nodes are either nodes that make a decision or leafs which constantly predict the same result\n",
    "        # We represent both possibilities using `ID3` objects and set `self.leaf` respectively\n",
    "        self.leaf = False\n",
    "        self.value = None\n",
    "        \n",
    "        self.children = {}\n",
    "        self.feature = 0\n",
    "        self.feature_split = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Creates a tree structure based on the passed data\n",
    "        \n",
    "        Arguments:\n",
    "            X: numpy array that contains the features in its rows\n",
    "            y: numpy array that contains the respective labels\n",
    "        \"\"\"\n",
    "        \n",
    "        self.counts = Counter(y)\n",
    "        self.most_common_label = self.counts.most_common()[0][0]\n",
    "        \n",
    "        # If there is only one class left, turn this node into a leaf\n",
    "        # and always return this one value\n",
    "        if len(set(y)) == 1:\n",
    "            self.leaf = True\n",
    "            self.value = y[0]\n",
    "        # If the tree is getting to deep, turn this node into a leaf\n",
    "        # and always predict the most common value\n",
    "        elif self.depth >= self.max_depth:\n",
    "            self.leaf = True\n",
    "            self.value = self.most_common_label\n",
    "        elif len({tuple(row) for row in X}) == 1:\n",
    "            self.leaf = True\n",
    "            self.value = self.most_common_label\n",
    "        # Otherwise, look for the most informative feature and do a split on its possible values\n",
    "        else:\n",
    "            self.feature, self.feature_split = self._choose_feature(X, y)\n",
    "            \n",
    "            # If no feature is informative enough, turn this node into a leaf\n",
    "            # and always predict the most common value\n",
    "            if self.feature is None:\n",
    "                self.leaf = True\n",
    "                self.value = self.most_common_label\n",
    "            else:\n",
    "                if self.feature in self.continuous:\n",
    "                    partition = self._partition_continuous(X, y, self.feature, self.feature_split)\n",
    "                else:\n",
    "                    partition = self._partition(X, y, self.feature)\n",
    "                    \n",
    "                if self._is_useful_partition(partition):\n",
    "                    for value, (Xi, yi) in partition.iteritems():\n",
    "                        child = C45(continuous=self.continuous, depth=self.depth+1)\n",
    "                        child.fit(Xi, yi)\n",
    "                        self.children[value] = child\n",
    "                else:\n",
    "                    self.leaf = True\n",
    "                    self.value = self.most_common_label\n",
    "    \n",
    "    def predict_single(self, x):\n",
    "        \"\"\"\n",
    "        Predict the class of a single data point x by either using the value encoded in a leaf\n",
    "        or by following the tree structure recursively until a leaf is reached\n",
    "        \n",
    "        Arguments:\n",
    "            x: individual data point\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.leaf:\n",
    "            return self.value\n",
    "        else:\n",
    "            value = x[self.feature]\n",
    "            \n",
    "            if self.feature in self.continuous:\n",
    "                if value <= self.feature_split:\n",
    "                    node = \"smaller\"\n",
    "                else:\n",
    "                    node = \"greater\"\n",
    "                    \n",
    "                return self.children[node].predict_single(x)\n",
    "            else:\n",
    "                if value in self.children:\n",
    "                    return self.children[value].predict_single(x)\n",
    "                else:\n",
    "                    return self.most_common_label\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the results for an entire dataset\n",
    "        \n",
    "        Arguments:\n",
    "            X: numpy array that contains each data point in a row\n",
    "        \"\"\"\n",
    "        \n",
    "        return [self.predict_single(x) for x in X]\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        Returns the accuracy for predicting the given dataset X\n",
    "        \"\"\"\n",
    "        \n",
    "        correct = sum(self.predict(X) == y)\n",
    "        return float(correct) / len(y)\n",
    "        \n",
    "    def _choose_feature(self, X, y):\n",
    "        \"\"\"\n",
    "        Finds the most informative feature to split on and returns its index.\n",
    "        If no feature is informative enough, `None` is returned\n",
    "        \"\"\"\n",
    "        \n",
    "        best_feature = 0\n",
    "        best_feature_gain = -float(\"inf\")\n",
    "        best_feature_split = None\n",
    "        \n",
    "        for i in range(X.shape[1]):\n",
    "            gain, split = self._information_gain(X, y, i)\n",
    "            #print \"feature %d has gain %.2f\" % (i, gain)\n",
    "\n",
    "            if gain > best_feature_gain:\n",
    "                best_feature = i\n",
    "                best_feature_gain = gain\n",
    "                best_feature_split = split\n",
    "                        \n",
    "        if best_feature_gain < self.min_gain:\n",
    "            best_feature = None\n",
    "            \n",
    "        return best_feature, best_feature_split\n",
    "        \n",
    "    def _information_gain(self, X, y, feature):\n",
    "        if feature in self.continuous:\n",
    "            max_gain, best_split = self._information_gain_continuous(X, y, feature)\n",
    "            return max_gain, best_split\n",
    "        else:\n",
    "            return self._information_gain_discrete(X, y, feature), 0\n",
    "    \n",
    "    def _information_gain_continuous(self, X, y, feature):\n",
    "        \"\"\"\n",
    "        Calculates the information gain achieved by splitting on the given feature\n",
    "        \"\"\"\n",
    "        \n",
    "        data, splits = self._get_continuous_splits(X, y, feature)\n",
    "        \n",
    "        old_entropy = self._entropy(y)\n",
    "        \n",
    "        max_gain = -float(\"inf\")\n",
    "        best_split = None\n",
    "        \n",
    "        for split in splits:\n",
    "            smaller = [yi for (xi, yi) in data if xi <= split]\n",
    "            greater = [yi for (xi, yi) in data if xi > split]\n",
    "                        \n",
    "            ratio_smaller = float(len(smaller)) / len(data)\n",
    "            \n",
    "            new_entropy = ratio_smaller * self._entropy(smaller) + (1 - ratio_smaller) * self._entropy(greater)\n",
    "            \n",
    "            result = old_entropy - new_entropy\n",
    "            \n",
    "            if result > max_gain:\n",
    "                best_split = split\n",
    "                max_gain = result\n",
    "        \n",
    "        return max_gain, best_split\n",
    "    \n",
    "    def _information_gain_discrete(self, X, y, feature):\n",
    "        \"\"\"\n",
    "        Calculates the information gain achieved by splitting on the given feature\n",
    "        \"\"\"\n",
    "        \n",
    "        result = self._entropy(y)\n",
    "        \n",
    "        summed = 0\n",
    "        \n",
    "        for value, (Xi, yi) in self._partition(X, y, feature).iteritems():\n",
    "            summed += float(len(yi)) / len(y) * self._entropy(yi)\n",
    "        \n",
    "        result -= summed\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def _entropy(self, X):\n",
    "        \"\"\"\n",
    "        Calculates the Shannon entropy on the given data X\n",
    "        \n",
    "        Arguments:\n",
    "            X: An iterable for feature values. Usually, this is now a 1D list\n",
    "        \"\"\"\n",
    "        \n",
    "        summed = 0\n",
    "        counter = Counter(X)\n",
    "\n",
    "        for value in counter:\n",
    "            count = counter[value]\n",
    "            px = count / float(len(X))\n",
    "            summed += px * logarithm(1. / px, 2)\n",
    "        \n",
    "        return summed\n",
    "    \n",
    "    def _partition(self, X, y, feature):\n",
    "        \"\"\"\n",
    "        Partitioning is a common operation needed for decision trees (or search trees).\n",
    "        Here, a partitioning is represented by a dictionary. The keys are values that the feature\n",
    "        can take. Under each key, we save a tuple (Xi, yi) that represents all data points (and their labels)\n",
    "        that have the respective value in the specified feature.\n",
    "        \"\"\"\n",
    "        \n",
    "        partition = defaultdict(lambda: ([], []))\n",
    "        \n",
    "        for Xi, yi in zip(X, y):\n",
    "            bucket = Xi[feature]\n",
    "            partition[bucket][0].append(Xi)\n",
    "            partition[bucket][1].append(yi)\n",
    "        \n",
    "        partition = dict(partition)\n",
    "            \n",
    "        for feature, (Xi, yi) in partition.iteritems():\n",
    "            partition[feature] = (np.array(Xi), np.array(yi))\n",
    "            \n",
    "        return partition\n",
    "    \n",
    "    def _partition_continuous(self, X, y, feature, split):\n",
    "        xi = X[:, feature]\n",
    "        smaller = xi <= split\n",
    "        \n",
    "        partition = {\n",
    "            \"smaller\": (X[smaller], y[smaller]),\n",
    "            \"greater\": (X[~smaller], y[~smaller])\n",
    "        }\n",
    "        \n",
    "        return partition\n",
    "    \n",
    "    def _get_continuous_splits(self, X, y, feature):\n",
    "        yi = y\n",
    "        xi = X[:, feature]\n",
    "        \n",
    "        datai = sorted(zip(xi, yi), key=itemgetter(0, 1))\n",
    "\n",
    "        splits = []\n",
    "\n",
    "        xs = []\n",
    "        ys = []\n",
    "        last_x = None\n",
    "\n",
    "        for xj, yj in datai:\n",
    "            if xj == last_x:\n",
    "                xs[-1].append(xj)\n",
    "                ys[-1].add(yj)\n",
    "            else:\n",
    "                xs.append([xj])\n",
    "                ys.append({yj})\n",
    "\n",
    "            last_x = xj\n",
    "\n",
    "        last_label = None\n",
    "\n",
    "        for xj, yj in zip(xs, ys):\n",
    "            if len(yj) == 1 and list(yj)[0] == last_label:\n",
    "                splits[-1] += xj\n",
    "            else:\n",
    "                splits.append(xj)\n",
    "\n",
    "            if len(yj) == 1:\n",
    "                last_label = list(yj)[0]\n",
    "            else:\n",
    "                last_label = None\n",
    "\n",
    "        splits = [mean(vals) for vals in splits]\n",
    "        \n",
    "        return datai, splits\n",
    "    \n",
    "    def _is_useful_partition(self, partition):\n",
    "        num_useful = 0\n",
    "        \n",
    "        for value, (Xi, yi) in partition.iteritems():\n",
    "            if len(yi) > 0:\n",
    "                num_useful += 1\n",
    "                \n",
    "        return num_useful >= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Pclass  Sex  Age  SibSp  Parch  Fare  Embarked\n",
      "PassengerId                                                \n",
      "1                 2    1   28      1      0    18         3\n",
      "2                 0    0   51      1      0   207         1\n",
      "3                 2    0   34      0      0    41         3\n",
      "4                 0    0   47      1      0   189         3\n",
      "5                 2    1   47      0      0    43         3\n"
     ]
    }
   ],
   "source": [
    "data = DataFrame.from_csv(\"./titanic/train.csv\")\n",
    "y = data[\"Survived\"].as_matrix()\n",
    "X = preprocess(data, encode_labels=True, impute=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "clf = C45(continuous={2, 5})\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy = 0.98736\n",
      "test accuracy = 0.76536\n"
     ]
    }
   ],
   "source": [
    "print \"train accuracy = %.5f\" % clf.score(X_train, y_train)\n",
    "print \"test accuracy = %.5f\" % clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0\n",
      "2\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "child = clf\n",
    "print clf.feature\n",
    "\n",
    "while len(child.children) > 0:\n",
    "    child = child.children[child.children.keys()[0]]\n",
    "    print child.feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.children[0.0].feature_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy = 0.98736\n",
      "test accuracy = 0.81564\n"
     ]
    }
   ],
   "source": [
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "print \"train accuracy = %.5f\" % clf.score(X_train, y_train)\n",
    "print \"test accuracy = %.5f\" % clf.score(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
