{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C4.5\n",
    "\n",
    "To improve on the shortcomings of ID3, we'll implement C4.5 and its main ideas:\n",
    "\n",
    "- Splitting continuous features smarter\n",
    "- Handling missing values directly\n",
    "- Growing the tree as far as possible and pruning it later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions\n",
    "\n",
    "A few general functions are needed for C4.5. We'll define them here because they might come in handy in another situation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mean(xs):\n",
    "    return float(sum(xs)) / len(xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def isnan(val):\n",
    "     return type(val) == float and math.isnan(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also want to use this function on numpy arrays because `np.isnan` doesn't work for some datatypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "isnan = np.vectorize(isnan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C4.5 implementation\n",
    "\n",
    "Generally, this is based on the ID3 implementation, but a lot of new stuff is added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "from math import log as logarithm\n",
    "from operator import itemgetter\n",
    "import Queue\n",
    "    \n",
    "class C45:\n",
    "    def __init__(self, max_depth=float(\"inf\"), min_gain=0, continuous={}, depth=0):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            max_depth: After eaching this depth, the current node is turned into a leaf which predicts\n",
    "                the most common label. This limits the capacity of the classifier and helps combat overfitting\n",
    "            min_gain: The minimum gain a split has to yield. Again, this helps overfitting\n",
    "            depth: Let's the current node know how deep it is into the tree, users usually don't need to set this\n",
    "        \"\"\"\n",
    "        \n",
    "        self.depth = depth\n",
    "        self.max_depth = max_depth\n",
    "        self.min_gain = min_gain\n",
    "        self.continuous = continuous\n",
    "        \n",
    "        # ID3 nodes are either nodes that make a decision or leafs which constantly predict the same result\n",
    "        # We represent both possibilities using `ID3` objects and set `self.leaf` respectively\n",
    "        self.leaf = False\n",
    "        self.value = None\n",
    "        \n",
    "        self.children = {}\n",
    "        self.feature = 0\n",
    "        self.feature_split = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Creates a tree structure based on the passed data\n",
    "        \n",
    "        Arguments:\n",
    "            X: numpy array that contains the features in its rows\n",
    "            y: numpy array that contains the respective labels\n",
    "        \"\"\"\n",
    "        \n",
    "        self.counts = Counter(y)\n",
    "        self.most_common_label = self.counts.most_common()[0][0]\n",
    "        \n",
    "        # If there is only one class left, turn this node into a leaf\n",
    "        # and always return this one value\n",
    "        if len(set(y)) == 1:\n",
    "            self.leaf = True\n",
    "            self.value = y[0]\n",
    "        # If the tree is getting to deep, turn this node into a leaf\n",
    "        # and always predict the most common value\n",
    "        elif self.depth >= self.max_depth:\n",
    "            self.leaf = True\n",
    "            self.value = self.most_common_label\n",
    "        elif len({tuple(row) for row in X}) == 1:\n",
    "            self.leaf = True\n",
    "            self.value = self.most_common_label\n",
    "        # Otherwise, look for the most informative feature and do a split on its possible values\n",
    "        else:\n",
    "            self.feature, self.feature_split = self._choose_feature(X, y)\n",
    "            \n",
    "            # If no feature is informative enough, turn this node into a leaf\n",
    "            # and always predict the most common value\n",
    "            if self.feature is None:\n",
    "                self.leaf = True\n",
    "                self.value = self.most_common_label\n",
    "            else:\n",
    "                if self.feature in self.continuous:\n",
    "                    partition = self._partition_continuous(X, y, self.feature, self.feature_split)\n",
    "                else:\n",
    "                    partition = self._partition(X, y, self.feature)\n",
    "                    \n",
    "                if self._is_useful_partition(partition):\n",
    "                    self._save_partition_proportions(partition)\n",
    "                    \n",
    "                    for value, (Xi, yi) in partition.iteritems():\n",
    "                        child = C45(continuous=self.continuous, depth=self.depth+1, max_depth=self.max_depth)\n",
    "                        child.fit(Xi, yi)\n",
    "                        self.children[value] = child\n",
    "                else:\n",
    "                    self.leaf = True\n",
    "                    self.value = self.most_common_label\n",
    "    \n",
    "    def predict_single(self, x):\n",
    "        \"\"\"\n",
    "        Predict the class of a single data point x by either using the value encoded in a leaf\n",
    "        or by following the tree structure recursively until a leaf is reached\n",
    "        \n",
    "        Arguments:\n",
    "            x: individual data point\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.leaf:\n",
    "            return self.value\n",
    "        else:\n",
    "            value = x[self.feature]\n",
    "            \n",
    "            if isnan(value):\n",
    "                return self._get_random_child_node().predict_single(x)\n",
    "            elif self.feature in self.continuous:\n",
    "                return self._predict_single_continuous(x, value)\n",
    "            else:\n",
    "                return self._predict_single_discrete(x, value)\n",
    "                \n",
    "    def _predict_single_discrete(self, x, value):\n",
    "        if value in self.children:\n",
    "            return self.children[value].predict_single(x)\n",
    "        else:\n",
    "            return self.most_common_label\n",
    "        \n",
    "    def _predict_single_continuous(self, x, value):\n",
    "        if value <= self.feature_split:\n",
    "            node = \"smaller\"\n",
    "        else:\n",
    "            node = \"greater\"\n",
    "\n",
    "        return self.children[node].predict_single(x)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the results for an entire dataset\n",
    "        \n",
    "        Arguments:\n",
    "            X: numpy array that contains each data point in a row\n",
    "        \"\"\"\n",
    "        \n",
    "        return [self.predict_single(x) for x in X]\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        Returns the accuracy for predicting the given dataset X\n",
    "        \"\"\"\n",
    "        \n",
    "        correct = sum(self.predict(X) == y)\n",
    "        return float(correct) / len(y)\n",
    "        \n",
    "    def _choose_feature(self, X, y):\n",
    "        \"\"\"\n",
    "        Finds the most informative feature to split on and returns its index.\n",
    "        If no feature is informative enough, `None` is returned\n",
    "        \"\"\"\n",
    "        \n",
    "        best_feature = 0\n",
    "        best_feature_gain = -float(\"inf\")\n",
    "        best_feature_split = None\n",
    "        \n",
    "        for i in range(X.shape[1]):\n",
    "            gain, split = self._information_gain(X, y, i)\n",
    "\n",
    "            if gain > best_feature_gain:\n",
    "                best_feature = i\n",
    "                best_feature_gain = gain\n",
    "                best_feature_split = split\n",
    "                        \n",
    "        if best_feature_gain < self.min_gain:\n",
    "            best_feature = None\n",
    "            \n",
    "        self.gain = best_feature_gain\n",
    "            \n",
    "        return best_feature, best_feature_split\n",
    "        \n",
    "    def _information_gain(self, X, y, feature):\n",
    "        if feature in self.continuous:\n",
    "            max_gain, best_split = self._information_gain_continuous(X, y, feature)\n",
    "            return max_gain, best_split\n",
    "        else:\n",
    "            return self._information_gain_discrete(X, y, feature), 0\n",
    "    \n",
    "    def _information_gain_continuous(self, X, y, feature):\n",
    "        \"\"\"\n",
    "        Calculates the information gain achieved by splitting on the given feature\n",
    "        \"\"\"\n",
    "        \n",
    "        data, splits = self._get_continuous_splits(X, y, feature)\n",
    "        \n",
    "        old_entropy = self._entropy(y)\n",
    "        \n",
    "        max_gain = -float(\"inf\")\n",
    "        best_split = None\n",
    "        \n",
    "        for split in splits:\n",
    "            smaller = [yi for (xi, yi) in data if xi <= split]\n",
    "            greater = [yi for (xi, yi) in data if xi > split]\n",
    "                        \n",
    "            ratio_smaller = float(len(smaller)) / len(data)\n",
    "            \n",
    "            new_entropy = ratio_smaller * self._entropy(smaller) + (1 - ratio_smaller) * self._entropy(greater)\n",
    "            \n",
    "            result = old_entropy - new_entropy\n",
    "            \n",
    "            if result > max_gain:\n",
    "                best_split = split\n",
    "                max_gain = result\n",
    "        \n",
    "        return max_gain, best_split\n",
    "    \n",
    "    def _information_gain_discrete(self, X, y, feature):\n",
    "        \"\"\"\n",
    "        Calculates the information gain achieved by splitting on the given feature\n",
    "        \"\"\"\n",
    "        \n",
    "        result = self._entropy(y)\n",
    "        \n",
    "        summed = 0\n",
    "        \n",
    "        for value, (Xi, yi) in self._partition(X, y, feature).iteritems():\n",
    "            # Missing values should be ignored for computing the entropy\n",
    "            if not isnan(value):\n",
    "                summed += float(len(yi)) / len(y) * self._entropy(yi)\n",
    "        \n",
    "        result -= summed\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def _entropy(self, X):\n",
    "        \"\"\"\n",
    "        Calculates the Shannon entropy on the given data X\n",
    "        \n",
    "        Arguments:\n",
    "            X: An iterable for feature values. Usually, this is now a 1D list\n",
    "        \"\"\"\n",
    "        \n",
    "        summed = 0\n",
    "        counter = Counter(X)\n",
    "\n",
    "        for value in counter:\n",
    "            count = counter[value]\n",
    "            px = count / float(len(X))\n",
    "            summed += px * logarithm(1. / px, 2)\n",
    "        \n",
    "        return summed\n",
    "    \n",
    "    def _partition(self, X, y, feature):\n",
    "        \"\"\"\n",
    "        Partitioning is a common operation needed for decision trees (or search trees).\n",
    "        Here, a partitioning is represented by a dictionary. The keys are values that the feature\n",
    "        can take. Under each key, we save a tuple (Xi, yi) that represents all data points (and their labels)\n",
    "        that have the respective value in the specified feature.\n",
    "        \"\"\"\n",
    "        \n",
    "        partition = defaultdict(lambda: ([], []))\n",
    "        \n",
    "        for Xi, yi in zip(X, y):\n",
    "            bucket = Xi[feature]\n",
    "            \n",
    "            partition[bucket][0].append(Xi)\n",
    "            partition[bucket][1].append(yi)\n",
    "        \n",
    "        partition = dict(partition)\n",
    "            \n",
    "        for feature, (Xi, yi) in partition.iteritems():\n",
    "            partition[feature] = (np.array(Xi), np.array(yi))\n",
    "            \n",
    "        return partition\n",
    "    \n",
    "    def _partition_continuous(self, X, y, feature, split):\n",
    "        xi = X[:, feature]\n",
    "        smaller = xi <= split\n",
    "        greater = xi > split\n",
    "        unknown = isnan(xi)\n",
    "        \n",
    "        ratio_smaller = sum(smaller) / float(sum(smaller) + sum(greater))\n",
    "        \n",
    "        unknown = np.where(unknown)[0]\n",
    "        np.random.shuffle(unknown)\n",
    "                \n",
    "        #num_first = int(ratio_smaller * len(unknown))\n",
    "        #smaller[unknown[:num_first]] = True\n",
    "        #greater[unknown[num_first:]] = True\n",
    "                \n",
    "        greater[unknown] = True\n",
    "        #smaller[unknown[:len(unknown)/2]] = True\n",
    "        #greater[unknown[len(unknown)/2:]] = True\n",
    "        \n",
    "        num_smaller = sum(smaller)\n",
    "        num_greater = sum(greater)\n",
    "        \n",
    "        #for i in unknown:\n",
    "        #    if num_smaller < num_greater:\n",
    "        #        smaller[i] = True\n",
    "        #        num_smaller += 1\n",
    "        #    else:\n",
    "        #        greater[i] = True\n",
    "        #        num_greater += 1\n",
    "                        \n",
    "        partition = {\n",
    "            \"smaller\": (X[smaller], y[smaller]),\n",
    "            \"greater\": (X[greater], y[greater])\n",
    "        }\n",
    "        \n",
    "        return partition\n",
    "    \n",
    "    def _get_continuous_splits(self, X, y, feature):\n",
    "        yi = y\n",
    "        xi = X[:, feature]\n",
    "        \n",
    "        datai = sorted(zip(xi, yi), key=itemgetter(0, 1))\n",
    "\n",
    "        splits = []\n",
    "\n",
    "        xs = []\n",
    "        ys = []\n",
    "        last_x = None\n",
    "\n",
    "        for xj, yj in datai:\n",
    "            # Missing values can't be used to find good thresholds\n",
    "            if isnan(xj):\n",
    "                continue\n",
    "                \n",
    "            if xj == last_x:\n",
    "                xs[-1].append(xj)\n",
    "                ys[-1].add(yj)\n",
    "            else:\n",
    "                xs.append([xj])\n",
    "                ys.append({yj})\n",
    "\n",
    "            last_x = xj\n",
    "\n",
    "        last_label = None\n",
    "\n",
    "        for xj, yj in zip(xs, ys):\n",
    "            if len(yj) == 1 and list(yj)[0] == last_label:\n",
    "                splits[-1] += xj\n",
    "            else:\n",
    "                splits.append(xj)\n",
    "\n",
    "            if len(yj) == 1:\n",
    "                last_label = list(yj)[0]\n",
    "            else:\n",
    "                last_label = None\n",
    "\n",
    "        splits = [mean(vals) for vals in splits]\n",
    "        \n",
    "        return datai, splits\n",
    "    \n",
    "    def _is_useful_partition(self, partition):\n",
    "        num_useful = 0\n",
    "        \n",
    "        for value, (Xi, yi) in partition.iteritems():\n",
    "            if len(yi) > 0:\n",
    "                num_useful += 1\n",
    "                \n",
    "        return num_useful >= 2\n",
    "    \n",
    "    def _save_partition_proportions(self, partition):\n",
    "        occurences = {}\n",
    "        \n",
    "        for child, (xj, xi) in partition.iteritems():\n",
    "            occurences[child] = len(xj)\n",
    "            \n",
    "        total = float(sum(occurences.values()))\n",
    "            \n",
    "        self.children_probs = { child: occ / total for child, occ in occurences.iteritems() }\n",
    "    \n",
    "    def _get_random_child_node(self):\n",
    "        name = np.random.choice(self.children_probs.keys(), p=self.children_probs.values())\n",
    "        return self.children[name]\n",
    "    \n",
    "    def prune(self, X_val, y_val):\n",
    "        old_score = self.score(X_val, y_val)\n",
    "        pruned = 0\n",
    "        not_pruned = 0\n",
    "        \n",
    "        for node in reversed(self._bfs()):\n",
    "            if not node.leaf:\n",
    "                node._make_leaf()\n",
    "                score = self.score(X_val, y_val)\n",
    "                \n",
    "                if old_score > score:\n",
    "                    node._make_internal()\n",
    "                    not_pruned += 1\n",
    "                else:\n",
    "                    old_score = score\n",
    "                    pruned += 1\n",
    "                    \n",
    "        return not_pruned, pruned\n",
    "    \n",
    "    def _bfs(self):\n",
    "        queue = Queue.Queue()\n",
    "        node = self\n",
    "        \n",
    "        queue.put(node)\n",
    "        result = [node]\n",
    "        \n",
    "        while not queue.empty():\n",
    "            if not node.leaf:\n",
    "                for _, child in node.children.iteritems():\n",
    "                    queue.put(child)\n",
    "                    \n",
    "            node = queue.get()\n",
    "            result.append(node)\n",
    "            \n",
    "        return result\n",
    "    \n",
    "    def _make_leaf(self):\n",
    "        self.leaf = True\n",
    "        self.value = self.most_common_label\n",
    "        \n",
    "    def _make_internal(self):\n",
    "        self.leaf = False\n",
    "        self.value = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Titanic dataset\n",
    "\n",
    "This is the same dataset as in the ID3 notebook, but now we can use continuous features, too.\n",
    "\n",
    "The head of the dataset is always printed, to show how the data we are working on looks like.\n",
    "Depending on the exact C4.5 functionality being used, we might want to encode labels or impute values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "from sklearn.preprocessing import Imputer, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(data, encode_labels=False, impute=False):\n",
    "    X = data.drop([\"Survived\", \"Name\", \"Ticket\", \"Cabin\"], 1)    \n",
    "    \n",
    "    if encode_labels: # for sklearn\n",
    "        X = X.apply(LabelEncoder().fit_transform)\n",
    "    \n",
    "    print X.head(10)\n",
    "    \n",
    "    X = X.as_matrix()\n",
    "    \n",
    "    if impute:\n",
    "        X = Imputer().fit_transform(X)\n",
    "            \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = DataFrame.from_csv(\"./titanic/train.csv\")\n",
    "y = data[\"Survived\"].as_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous features\n",
    "\n",
    "Missing values are imputed, and labels are encoded numerically. In contrast to ID3, we can now use continuous variables like age or the price paid. We need to tell C4.5 which variables are continuous, because this is hard (or error-prone) to automatically derive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Pclass  Sex  Age  SibSp  Parch  Fare  Embarked\n",
      "PassengerId                                                \n",
      "1                 2    1   28      1      0    18         3\n",
      "2                 0    0   51      1      0   207         1\n",
      "3                 2    0   34      0      0    41         3\n",
      "4                 0    0   47      1      0   189         3\n",
      "5                 2    1   47      0      0    43         3\n",
      "6                 2    1  110      0      0    51         2\n",
      "7                 0    1   69      0      0   186         3\n",
      "8                 2    1    6      3      1   124         3\n",
      "9                 2    0   35      0      2    74         3\n",
      "10                1    0   18      1      0   154         1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/numpy/lib/arraysetops.py:200: FutureWarning: numpy not_equal will not check object identity in the future. The comparison did not return the same result as suggested by the identity (`is`)) and will change.\n",
      "  flag = np.concatenate(([True], aux[1:] != aux[:-1]))\n"
     ]
    }
   ],
   "source": [
    "X = preprocess(data, encode_labels=True, impute=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy = 0.98736\n",
      "test accuracy = 0.76536\n"
     ]
    }
   ],
   "source": [
    "clf = C45(continuous={2, 5})\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print \"train accuracy = %.5f\" % clf.score(X_train, y_train)\n",
    "print \"test accuracy = %.5f\" % clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, training accuracy is way up compared to ID3, but test accuracy actually got worse. This is because it's now much easier to overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing values\n",
    "\n",
    "As explained in the report, one major feature of C4.5 is that we don't have to manually impute missing values. So now, let's set the preprocessor up to not impute, and see how C4.5 deals with that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Pclass     Sex   Age  SibSp  Parch     Fare Embarked\n",
      "PassengerId                                                      \n",
      "1                 3    male  22.0      1      0   7.2500        S\n",
      "2                 1  female  38.0      1      0  71.2833        C\n",
      "3                 3  female  26.0      0      0   7.9250        S\n",
      "4                 1  female  35.0      1      0  53.1000        S\n",
      "5                 3    male  35.0      0      0   8.0500        S\n",
      "6                 3    male   NaN      0      0   8.4583        Q\n",
      "7                 1    male  54.0      0      0  51.8625        S\n",
      "8                 3    male   2.0      3      1  21.0750        S\n",
      "9                 3  female  27.0      0      2  11.1333        S\n",
      "10                2  female  14.0      1      0  30.0708        C\n"
     ]
    }
   ],
   "source": [
    "X = preprocess(data)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy = 0.93539\n",
      "test accuracy = 0.77095\n"
     ]
    }
   ],
   "source": [
    "clf = C45(continuous={2, 5})\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print \"train accuracy = %.5f\" % clf.score(X_train, y_train)\n",
    "print \"test accuracy = %.5f\" % clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7793296089385477"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean([clf.score(X_test, y_test) for _ in range(100)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, training accuracy got down. This is because it's harder to describe data with missing values. On the other hand, test accuracy is now improved, which is often our main goal.\n",
    "\n",
    "Because there's now some randomness involved, we compute the mean of many test scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pruning\n",
    "\n",
    "Next, we will grow the tree as far as possible and then prune the parts that don't help.\n",
    "\n",
    "We will split our training set into a dataset that is used directly for training, and a validation set that is used to evaluate the nodes in the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_sub, X_val, y_train_sub, y_val = train_test_split(X_train, y_train, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(308, 39)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = C45(continuous={2, 5})\n",
    "clf.fit(X_train_sub, y_train_sub)\n",
    "clf.prune(X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The return value tells us that 23 nodes were pruned and 314 nodes were kept. This node count excludes existing leafs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy = 0.83567\n",
      "test accuracy = 0.79888\n"
     ]
    }
   ],
   "source": [
    "print \"train accuracy = %.5f\" % clf.score(X_train, y_train)\n",
    "print \"test accuracy = %.5f\" % clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8058100558659214"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean([clf.score(X_test, y_test) for _ in range(100)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, this improves the accuracy a bit more. However, we are still below the accuracy we get using the simple ID3 model, where we do some prior feature selection (i.e. exclude the continuous features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy = 0.91433\n",
      "test accuracy = 0.83240\n"
     ]
    }
   ],
   "source": [
    "clf = C45(continuous={2, 5})\n",
    "clf.fit(X_train, y_train)\n",
    "clf.prune(X_test, y_test)\n",
    "\n",
    "print \"train accuracy = %.5f\" % clf.score(X_train, y_train)\n",
    "print \"test accuracy = %.5f\" % clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, when using the test set for pruning, we get much improved test accuracy. However, this is cheating because we shouldn't use test data for model selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drawing\n",
    "\n",
    "Like in the ID3 notebook, we want to draw our results. We made some modifications to the draw functions, to draw continuous splits in a nice way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def setNodeId(depth,index=0):\n",
    "    return str(int(depth)) + str(int(index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_content(node, result_list):\n",
    "    i = 0\n",
    "    node_txt = ''\n",
    "    while i < len(node.counts.keys()):\n",
    "        tmp_result = ''\n",
    "        \n",
    "        number = node.counts[node.counts.keys()[i]]                        \n",
    "        tmp_result = result_list[node.counts.keys()[i]] + ': ' + str(number) + '\\n'\n",
    "        \n",
    "        node_txt += tmp_result\n",
    "                        \n",
    "        i += 1\n",
    "    return node_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import Queue\n",
    "import pydot\n",
    "\n",
    "\n",
    "\n",
    "def draw(node,feature_list, result_list, path):\n",
    "    \n",
    "    graph = pydot.Dot(graph_type='graph')\n",
    "    \n",
    "    cid = 0\n",
    "    \n",
    "    que = Queue.Queue()\n",
    "    \n",
    "    node.Id = setNodeId(node.depth)\n",
    "    que.put(node)\n",
    "    \n",
    "    while(que.qsize() > 0):\n",
    "        \n",
    "        node = que.get()\n",
    "        \n",
    "        feature = feature_list[node.feature]\n",
    "        \n",
    "        node_txt =  feature + '\\n' + show_content(node, result_list)\n",
    "        \n",
    "        graph.add_node(pydot.Node(node.Id, label = node_txt))\n",
    "        \n",
    "        \n",
    "        for index in node.children.keys():\n",
    "            if node.children[index].leaf == True:\n",
    "                if len(node.children[index].counts.keys()) == 1:\n",
    "                    edge_txt = ''\n",
    "                    \n",
    "                    node.children[index].Id = setNodeId(node.children[index].depth, cid)                \n",
    "                    \n",
    "                    value = node.children[index].counts[node.children[index].counts.keys()[0]]\n",
    "                    result = result_list[node.children[index].counts.keys()[0]]\n",
    "                                        \n",
    "                    graph.add_node(pydot.Node(node.children[index].Id, label = result + \"\\n\" + str(value), shape = 'box'))\n",
    "                    \n",
    "                    if node.feature not in node.continuous:\n",
    "                        edge_txt = str(index)\n",
    "                    else:\n",
    "                        if str(index) == \"smaller\":\n",
    "                            edge_txt = u'≤' + str(node.feature_split)\n",
    "                        else:\n",
    "                            edge_txt = '>' + str(node.feature_split)\n",
    "                    \n",
    "                    edge = pydot.Edge(node.Id, node.children[index].Id, label= edge_txt)\n",
    "                    graph.add_edge(edge)\n",
    "                    \n",
    "                    cid += 1\n",
    "                else:\n",
    "                    edge_txt = ''\n",
    "                    node_txt = show_content(node.children[index], result_list)\n",
    "                    \n",
    "                    node.children[index].Id = setNodeId(node.children[index].depth, cid)\n",
    "                    graph.add_node(pydot.Node(node.children[index].Id, label = node_txt, shape = 'box'))\n",
    "                    \n",
    "                    if node.feature not in node.continuous:\n",
    "                        edge_txt = str(index)\n",
    "                    else:\n",
    "                        if str(index) == \"smaller\":\n",
    "                            edge_txt = u'≤' + str(node.feature_split)\n",
    "                        else:\n",
    "                            edge_txt = '>' + str(node.feature_split)\n",
    "                    \n",
    "                    edge = pydot.Edge(node.Id, node.children[index].Id, label= edge_txt)\n",
    "                    graph.add_edge(edge)\n",
    "                    \n",
    "                    cid += 1\n",
    "                    \n",
    "            else:\n",
    "                edge_txt = ''\n",
    "                node.children[index].Id = setNodeId(node.children[index].depth, cid)                \n",
    "                \n",
    "                if node.feature not in node.continuous:\n",
    "                    edge_txt = str(index)\n",
    "                else:\n",
    "                    if str(index) == \"smaller\":\n",
    "                        edge_txt = u'≤' + str(node.feature_split)\n",
    "                    else:\n",
    "                        edge_txt = '>' + str(node.feature_split)\n",
    "                \n",
    "                edge = pydot.Edge(node.Id, node.children[index].Id, label = edge_txt)\n",
    "                graph.add_edge(edge)\n",
    "                que.put(node.children[index])\n",
    "                cid += 1\n",
    "    \n",
    "    graph.write_png(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = C45(continuous={2, 5}, max_depth=2)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "feature_list = ['Pclass',  'Sex',  'Age',  'SibSp',  'Parch', 'Fare',  'Embarked']\n",
    "survive_list = ['Not Survived', 'Survived']\n",
    "draw(clf, feature_list, survive_list, path=\"c45-depth2.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = C45(continuous={2, 5}, max_depth=3)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "feature_list = ['Pclass',  'Sex',  'Age',  'SibSp',  'Parch', 'Fare',  'Embarked']\n",
    "survive_list = ['Not Survived', 'Survived']\n",
    "draw(clf, feature_list, survive_list, path=\"c45-depth3.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = C45(continuous={2, 5}, max_depth=4)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "feature_list = ['Pclass',  'Sex',  'Age',  'SibSp',  'Parch', 'Fare',  'Embarked']\n",
    "survive_list = ['Not Survived', 'Survived']\n",
    "draw(clf, feature_list, survive_list, path=\"c45-depth4.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking with sklearn\n",
    "\n",
    "Again, we can take a look at how well sklearn does on this dataset to see if we're totally off with our results.\n",
    "sklearn implements the CART algorithm for decision trees. Additionally, this implementation cannot deal with missing values, so we need to impute them and encode all labels numerically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Pclass  Sex  Age  SibSp  Parch  Fare  Embarked\n",
      "PassengerId                                                \n",
      "1                 2    1   28      1      0    18         3\n",
      "2                 0    0   51      1      0   207         1\n",
      "3                 2    0   34      0      0    41         3\n",
      "4                 0    0   47      1      0   189         3\n",
      "5                 2    1   47      0      0    43         3\n",
      "6                 2    1  110      0      0    51         2\n",
      "7                 0    1   69      0      0   186         3\n",
      "8                 2    1    6      3      1   124         3\n",
      "9                 2    0   35      0      2    74         3\n",
      "10                1    0   18      1      0   154         1\n"
     ]
    }
   ],
   "source": [
    "X = preprocess(data, encode_labels=True, impute=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy = 0.98736\n",
      "test accuracy = 0.80447\n"
     ]
    }
   ],
   "source": [
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "print \"train accuracy = %.5f\" % clf.score(X_train, y_train)\n",
    "print \"test accuracy = %.5f\" % clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, sklearn performs very similiarly in terms of test accuracy. We got a mean of `0.8058100558659214` using C4.5 without imputed values, which is just a little bit better than sklearn's performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8044692737430182"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean([clf.score(X_test, y_test) for _ in range(100)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
