{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ID3\n",
    "\n",
    "ID3 is a simple algorithm to create decision trees. It has several shortcomings, but still this is a good algorithm to start with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "from math import log as logarithm\n",
    "\n",
    "class ID3:\n",
    "    def __init__(self, max_depth=float(\"inf\"), min_gain=0, depth=0):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            max_depth: After eaching this depth, the current node is turned into a leaf which predicts\n",
    "                the most common label. This limits the capacity of the classifier and helps combat overfitting\n",
    "            min_gain: The minimum gain a split has to yield. Again, this helps overfitting\n",
    "            depth: Let's the current node know how deep it is into the tree, users usually don't need to set this\n",
    "        \"\"\"\n",
    "        \n",
    "        self.depth = depth\n",
    "        self.max_depth = max_depth\n",
    "        self.min_gain = min_gain\n",
    "        \n",
    "        # ID3 nodes are either nodes that make a decision or leafs which constantly predict the same result\n",
    "        # We represent both possibilities using `ID3` objects and set `self.leaf` respectively\n",
    "        self.leaf = False\n",
    "        self.value = None\n",
    "        \n",
    "        self.children = {}\n",
    "        self.feature = 0\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Creates a tree structure based on the passed data\n",
    "        \n",
    "        Arguments:\n",
    "            X: numpy array that contains the features in its rows\n",
    "            y: numpy array that contains the respective labels\n",
    "        \"\"\"\n",
    "        \n",
    "        self.counts = Counter(y)\n",
    "        self.most_common_label = self.counts.most_common()[0][0]\n",
    "        \n",
    "        # If there is only one class left, turn this node into a leaf\n",
    "        # and always return this one value\n",
    "        if len(set(y)) == 1:\n",
    "            self.leaf = True\n",
    "            self.value = y[0]\n",
    "        # If the tree is getting to deep, turn this node into a leaf\n",
    "        # and always predict the most common value\n",
    "        elif self.depth >= self.max_depth:\n",
    "            print \"Depth too large\"\n",
    "            self.leaf = True\n",
    "            self.value = self.most_common_label\n",
    "        # Otherwise, look for the most informative feature and do a split on its possible values\n",
    "        else:\n",
    "            self.feature = self._choose_feature(X, y)\n",
    "            \n",
    "            # If no feature is informative enough, turn this node into a leaf\n",
    "            # and always predict the most common value\n",
    "            if self.feature is None:\n",
    "                self.leaf = True\n",
    "                self.value = self.most_common_label\n",
    "            else:\n",
    "                for value, (Xi, yi) in self._partition(X, y, self.feature).iteritems():\n",
    "                    child = ID3(max_depth=self.max_depth, depth=self.depth+1)\n",
    "                    child.fit(Xi, yi)\n",
    "                    self.children[value] = child\n",
    "    \n",
    "    def predict_single(self, x):\n",
    "        \"\"\"\n",
    "        Predict the class of a single data point x by either using the value encoded in a leaf\n",
    "        or by following the tree structure recursively until a leaf is reached\n",
    "        \n",
    "        Arguments:\n",
    "            x: individual data point\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.leaf:\n",
    "            return self.value\n",
    "        else:\n",
    "            value = x[self.feature]\n",
    "            \n",
    "            if value in self.children:\n",
    "                return self.children[value].predict_single(x)\n",
    "            else:\n",
    "                return self.most_common_label\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the results for an entire dataset\n",
    "        \n",
    "        Arguments:\n",
    "            X: numpy array that contains each data point in a row\n",
    "        \"\"\"\n",
    "        \n",
    "        return [self.predict_single(x) for x in X]\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        Returns the accuracy for predicting the given dataset X\n",
    "        \"\"\"\n",
    "        \n",
    "        correct = sum(self.predict(X) == y)\n",
    "        return float(correct) / len(y)\n",
    "        \n",
    "    def _choose_feature(self, X, y):\n",
    "        \"\"\"\n",
    "        Finds the most informative feature to split on and returns its index.\n",
    "        If no feature is informative enough, `None` is returned\n",
    "        \"\"\"\n",
    "        \n",
    "        best_feature = 0\n",
    "        best_feature_gain = -float(\"inf\")\n",
    "        \n",
    "        for i in range(X.shape[1]):\n",
    "            gain = self._information_gain(X, y, i)\n",
    "                        \n",
    "            if gain > best_feature_gain:\n",
    "                best_feature = i\n",
    "                best_feature_gain = gain\n",
    "        \n",
    "        if best_feature_gain > self.min_gain:\n",
    "            return best_feature\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def _information_gain(self, X, y, feature):\n",
    "        \"\"\"\n",
    "        Calculates the information gain achieved by splitting on the given feature\n",
    "        \"\"\"\n",
    "        \n",
    "        result = self._entropy(y)\n",
    "        \n",
    "        summed = 0\n",
    "        \n",
    "        for value, (Xi, yi) in self._partition(X, y, feature).iteritems():\n",
    "            summed += float(len(yi)) / len(y) * self._entropy(yi)\n",
    "        \n",
    "        result -= summed\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def _entropy(self, X):\n",
    "        \"\"\"\n",
    "        Calculates the Shannon entropy on the given data X\n",
    "        \n",
    "        Arguments:\n",
    "            X: An iterable for feature values. Usually, this is now a 1D list\n",
    "        \"\"\"\n",
    "        \n",
    "        summed = 0\n",
    "        counter = Counter(X)\n",
    "\n",
    "        for value in counter:\n",
    "            count = counter[value]\n",
    "            px = count / float(len(X))\n",
    "            summed += px * logarithm(1. / px, 2)\n",
    "        \n",
    "        return summed\n",
    "    \n",
    "    def _partition(self, X, y, feature):\n",
    "        \"\"\"\n",
    "        Partitioning is a common operation needed for decision trees (or search trees).\n",
    "        Here, a partitioning is represented by a dictionary. The keys are values that the feature\n",
    "        can take. Under each key, we save a tuple (Xi, yi) that represents all data points (and their labels)\n",
    "        that have the respective value in the specified feature.\n",
    "        \"\"\"\n",
    "        \n",
    "        partition = defaultdict(lambda: ([], []))\n",
    "        \n",
    "        for Xi, yi in zip(X, y):\n",
    "            bucket = Xi[feature]\n",
    "            partition[bucket][0].append(Xi)\n",
    "            partition[bucket][1].append(yi)\n",
    "        \n",
    "        partition = dict(partition)\n",
    "            \n",
    "        for feature, (Xi, yi) in partition.iteritems():\n",
    "            partition[feature] = (np.array(Xi), np.array(yi))\n",
    "            \n",
    "        return partition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## xor dataset\n",
    "\n",
    "`xor` is an easy test case: It's not linearly separable but it's clear what the output should be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.array([\n",
    "    [0, 0],\n",
    "    [0, 1],\n",
    "    [1, 0],\n",
    "    [1, 1]\n",
    "])\n",
    "\n",
    "y = np.array([0, 1, 1, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to specify `min_gain < 0` because we need to take non informative splits in the beginning to get a better split later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = ID3(min_gain=-1)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, it works perfectly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting [0 0] as 0 (correct = 0)\n",
      "Predicting [0 1] as 1 (correct = 1)\n",
      "Predicting [1 0] as 1 (correct = 1)\n",
      "Predicting [1 1] as 0 (correct = 0)\n"
     ]
    }
   ],
   "source": [
    "for Xi, yi in zip(X, y):\n",
    "    print \"Predicting\", Xi, \"as\", clf.predict_single(Xi), \"(correct = %d)\" % yi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Titanic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "from sklearn.preprocessing import Imputer, LabelEncoder\n",
    "\n",
    "def preprocess(data, encode_labels=False):\n",
    "    X = data.drop([\"Survived\", \"Name\", \"Ticket\", \"Cabin\", \"Age\", \"Fare\"], 1)    \n",
    "    \n",
    "    if encode_labels: # for sklearn\n",
    "        X = X.apply(LabelEncoder().fit_transform)\n",
    "        \n",
    "    print X.head()\n",
    "    \n",
    "    return X.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Pclass     Sex  SibSp  Parch Embarked\n",
      "PassengerId                                       \n",
      "1                 3    male      1      0        S\n",
      "2                 1  female      1      0        C\n",
      "3                 3  female      0      0        S\n",
      "4                 1  female      1      0        S\n",
      "5                 3    male      0      0        S\n"
     ]
    }
   ],
   "source": [
    "data = DataFrame.from_csv(\"./titanic/train.csv\")\n",
    "y = data[\"Survived\"].as_matrix()\n",
    "X = preprocess(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "clf = ID3()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy = 0.84270\n",
      "test accuracy = 0.79330\n"
     ]
    }
   ],
   "source": [
    "print \"train accuracy = %.5f\" % clf.score(X_train, y_train)\n",
    "print \"test accuracy = %.5f\" % clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Drawing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def setNodeId(depth,index=0):\n",
    "    return str(int(depth)) + str(int(index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_content(node, result_list):\n",
    "    i = 0\n",
    "    node_txt = ''\n",
    "    while i < len(node.counts.keys()):\n",
    "        tmp_result = ''\n",
    "        \n",
    "        number = node.counts[node.counts.keys()[i]]                        \n",
    "        tmp_result = result_list[node.counts.keys()[i]] + ': ' + str(number) + '\\n'\n",
    "        \n",
    "        node_txt += tmp_result\n",
    "                        \n",
    "        i += 1\n",
    "    return node_txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import Queue\n",
    "import pydot\n",
    "\n",
    "graph = pydot.Dot(graph_type='graph')\n",
    "\n",
    "def draw(node,feature_list, result_list):\n",
    "    \n",
    "    \n",
    "    cid = 0\n",
    "    \n",
    "    que = Queue.Queue()\n",
    "    \n",
    "    node.Id = setNodeId(node.depth)\n",
    "    que.put(node)\n",
    "    \n",
    "    while(que.qsize() > 0):\n",
    "        \n",
    "        node = que.get()\n",
    "        \n",
    "        feature = feature_list[node.feature]\n",
    "        node_txt = feature + '\\n' + show_content(node, result_list)\n",
    "        \n",
    "        graph.add_node(pydot.Node(node.Id, label = node_txt))\n",
    "        \n",
    "        for index in node.children.keys():\n",
    "            if node.children[index].leaf == True:\n",
    "            \n",
    "                if len(node.children[index].counts.keys()) == 1:\n",
    "                    node.children[index].Id = setNodeId(node.children[index].depth, cid)                \n",
    "                    \n",
    "                    value = node.children[index].counts[node.children[index].counts.keys()[0]]\n",
    "                    result = result_list[node.children[index].counts.keys()[0]]\n",
    "                    \n",
    "                    graph.add_node(pydot.Node(node.children[index].Id, label = result + \"\\n\" + str(value), shape = 'box'))\n",
    "                    edge = pydot.Edge(node.Id, node.children[index].Id, label= str(index))\n",
    "                    graph.add_edge(edge)\n",
    "                    \n",
    "                    cid += 1\n",
    "                else:\n",
    "                    node_txt = show_content(node.children[index], result_list)\n",
    "                        \n",
    "                    node.children[index].Id = setNodeId(node.children[index].depth, cid)\n",
    "                    graph.add_node(pydot.Node(node.children[index].Id, label = node_txt, shape = 'box'))\n",
    "                    edge = pydot.Edge(node.Id, node.children[index].Id, label= str(index))\n",
    "                    graph.add_edge(edge)\n",
    "                    \n",
    "                    cid += 1\n",
    "                    \n",
    "                \n",
    "            else:\n",
    "                node.children[index].Id = setNodeId(node.children[index].depth, cid)                \n",
    "                \n",
    "                edge = pydot.Edge(node.Id, node.children[index].Id, label = str(index))\n",
    "                graph.add_edge(edge)\n",
    "                \n",
    "                que.put(node.children[index])\n",
    "                cid += 1\n",
    "    \n",
    "    graph.write_png('./id3_decision_tree.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_list = ['Pclass',  'Sex', 'SibSp', 'Parch', 'Embarked']\n",
    "survive_list = ['Not Survived', 'Survived']\n",
    "\n",
    "draw(clf, feature_list, survive_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### xor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.array([\n",
    "    [0, 0],\n",
    "    [0, 1],\n",
    "    [1, 0],\n",
    "    [1, 1]\n",
    "])\n",
    "\n",
    "y = np.array([0, 1, 1, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to specify `min_gain < 0` because we need to take non informative splits in the beginning to get a better split later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = ID3(min_gain=-1)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list = ['a', 'b']\n",
    "survive_list = ['false', 'true']\n",
    "\n",
    "draw(clf, feature_list, survive_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison to sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We get the exact same accuracy as sklearn!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Pclass  Sex  SibSp  Parch  Embarked\n",
      "PassengerId                                     \n",
      "1                 2    1      1      0         3\n",
      "2                 0    0      1      0         1\n",
      "3                 2    0      0      0         3\n",
      "4                 0    0      1      0         3\n",
      "5                 2    1      0      0         3\n",
      "train accuracy = 0.84270\n",
      "test accuracy = 0.79330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/numpy/lib/arraysetops.py:200: FutureWarning: numpy not_equal will not check object identity in the future. The comparison did not return the same result as suggested by the identity (`is`)) and will change.\n",
      "  flag = np.concatenate(([True], aux[1:] != aux[:-1]))\n"
     ]
    }
   ],
   "source": [
    "y = data[\"Survived\"].as_matrix()\n",
    "X = preprocess(data, encode_labels=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "print \"train accuracy = %.5f\" % clf.score(X_train, y_train)\n",
    "print \"test accuracy = %.5f\" % clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forest doesn't seem to be very useful here. Probably because it's hard to overfit without having information about age or the paid price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy = 0.84129\n",
      "test accuracy = 0.80447\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "print \"train accuracy = %.5f\" % clf.score(X_train, y_train)\n",
    "print \"test accuracy = %.5f\" % clf.score(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
